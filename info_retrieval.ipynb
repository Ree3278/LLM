{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"Climate change is accelerating faster than previously thought, \\\n",
    "      according to a new report from the Intergovernmental Panel on \\\n",
    "      Climate Change (IPCC). The report highlights that global \\\n",
    "      temperatures have risen by an average of 1.1°C since the late \\\n",
    "      19th century, with most of this increase occurring in the last 40 years. Rising sea levels, more frequent extreme weather events, and loss of biodiversity are some of the key impacts outlined in the report.\"\n",
    "d2 = \"The human genome consists of approximately 3 billion base pairs of DNA, which are organized into 23 pairs of chromosomes. Each gene contains the instructions necessary to build proteins, which are the molecular machines that carry out many functions in the body. The process of gene expression involves transcription, where DNA is copied into RNA, and translation, where RNA is used to build proteins.\"\n",
    "d3 = \"Person A: Hey, did you finish the report for the marketing team? \\\n",
    "      Person B: Not yet, I’m almost done. I just need to add a few more details. \\\n",
    "      Person A: Alright, make sure it’s submitted by 5 PM. \\\n",
    "      Person B: No problem, I’ve got it covered.\"\n",
    "d4 = \"n the year 1066, the Battle of Hastings took place in England, marking a pivotal moment in the country’s history. William the Conqueror, the Duke of Normandy, defeated King Harold II, which led to the Norman conquest of England. This event significantly influenced the culture, language, and governance of England.\"\n",
    "d5 = \"I recently bought the new XYZ smartphone, and I have to say, I’m really impressed. The camera quality is excellent, especially in low light. The battery life easily lasts a full day with heavy use, and the screen resolution is sharp and vibrant. My only complaint is the fingerprint sensor, which can be a bit finicky at times.\"\n",
    "\n",
    "\n",
    "docs = [d1, d2, d3, d4, d5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'used', 'camera', 'say', 'risen', 'more', 'battery', 'bought', 'rising', 'william', 'no', 'by', 'governance', '3', 'organized', 'outlined', 'problem', 'where', 'intergovernmental', 'are', 'moment', 'you', 'conquest', 'rna', 'billion', 'i', 'make', 'instructions', 'with', 'events', 'climate', 'out', 'proteins', 'it', 'details', 'year', 'late', 'vibrant', 'bit', 'a', 'covered', 'influenced', 'expression', 'n', 'country’s', 'my', 'few', 'only', 'from', 'took', 'culture', 'times', 'gene', 'translation', 'machines', 'just', 'really', 'yet', 'for', 'since', 'this', 'not', 'history', 'into', 'ipcc', 'involves', 'life', '40', 'and', 'harold', 'report', 'some', 'excellent', 'norman', 'duke', 'significantly', 'sensor', 'according', 'biodiversity', 'sure', '23', '1°c', '5', 'did', 'occurring', 'it’s', 'got', 'marking', 'low', 'of', 'base', 'defeated', 'full', 'loss', 'average', 'weather', 'i’m', 'battle', 'key', 'transcription', 'on', 'century', 'chromosomes', 'alright', 'approximately', 'dna', 'contains', 'highlights', 'marketing', 'language', 'heavy', 'than', 'can', 'increase', 'new', 'molecular', 'carry', 'b', 'thought', 'use', 'impacts', 'person', 'complaint', 'levels', 'need', 'king', 'easily', 'fingerprint', 'in', 'to', 'finish', 'temperatures', 'frequent', 'process', 'copied', 'team', '19th', 'pm', 'be', 'england', 'which', 'submitted', 'pivotal', 'previously', 'sharp', 'impressed', 'is', 'lasts', 'day', 'change', 'hey', 'at', 'normandy', '1', '1066', 'functions', 'extreme', 'done', 'most', 'have', 'accelerating', 'i’ve', 'xyz', 'pairs', 'led', 'faster', 'sea', 'build', 'hastings', 'genome', 'ii', 'event', 'the', 'almost', 'quality', 'especially', 'last', 'consists', 'body', 'finicky', 'many', 'an', 'years', 'necessary', 'light', 'resolution', 'that', 'place', 'add', 'conqueror', 'global', 'recently', 'smartphone', 'human', 'screen', 'panel', 'each'}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(docs:list[str]):\n",
    "    sep = r'[ \\n\\r\\t\\f,.\\(\\)\\?:]+'\n",
    "    pattern = re.compile(sep)\n",
    "    vocabs = set()\n",
    "    for d in docs:\n",
    "        words = pattern.split(d.strip().lower())\n",
    "        vocabs.update(words)\n",
    "    if '' in vocabs:\n",
    "        vocabs.remove('')\n",
    "    return vocabs\n",
    "\n",
    "#print(tokenize(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE (byte-pair encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A l r i ght ,  make  sure  it’s  submitted  by\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_pair_counts(tokenized_doc:list[str]):\n",
    "    pair_counts = Counter()\n",
    "    for word in tokenized_doc:\n",
    "        chars = word.split()\n",
    "        for i in range(len(chars)-1):\n",
    "            pair = (chars[i],chars[i+1])\n",
    "            pair_counts[pair] +=1\n",
    "    return pair_counts\n",
    "\n",
    "\n",
    "def BPE_training(docs:list[str], numMerges:int = 1000):\n",
    "    vocabs = set()\n",
    "    for d in docs:\n",
    "        vocabs.update(re.split(r\"\", d.strip().lower()))\n",
    "    if '' in vocabs:\n",
    "        vocabs.remove('')\n",
    "\n",
    "    word_tokens = []\n",
    "    for d in docs:\n",
    "        words = d.strip().lower().split()\n",
    "        word_tokens.extend([' '.join(w) for w in words])\n",
    "\n",
    "    merged_rule = []\n",
    "    for i in range(numMerges):\n",
    "        pair_counts = get_pair_counts(word_tokens)\n",
    "        if not pair_counts:\n",
    "            break\n",
    "        pair = pair_counts.most_common(1)[0][0]\n",
    "        vocabs.add(''.join(pair))\n",
    "\n",
    "        new_word_tokens = []\n",
    "        for word in word_tokens:\n",
    "            pattern = re.escape(\" \".join(pair))\n",
    "            merged_word = re.sub(pattern, \"\".join(pair), word)\n",
    "            new_word_tokens.append(merged_word)\n",
    "        \n",
    "        word_tokens = new_word_tokens\n",
    "        merged_rule.append(pair)\n",
    "    \n",
    "    \n",
    "    for word in word_tokens:\n",
    "        vocabs.update(word.split())\n",
    "\n",
    "    return vocabs, merged_rule\n",
    "\n",
    "\n",
    "\n",
    "def BPE_infer(phrase:str, merged_rule:list[tuple]):\n",
    "    tokenized_doc = [' '.join(w) for w in phrase.split()]\n",
    "    for pair in merged_rule:\n",
    "        pattern = re.escape(\" \".join(pair))\n",
    "        new_tokenized_doc = []\n",
    "        for word in tokenized_doc:\n",
    "            merged_word = re.sub(pattern, \"\".join(pair),word)\n",
    "            new_tokenized_doc.append(merged_word)\n",
    "            tokenized_doc = new_tokenized_doc\n",
    "    return \"  \".join(tokenized_doc)\n",
    "\n",
    "\n",
    "vocabs , merged_rule = BPE_training(docs)\n",
    "print(BPE_infer(\"Alright, make sure it’s submitted by\", merged_rule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>475</td>\n",
       "      <td>402</td>\n",
       "      <td>254</td>\n",
       "      <td>315</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>camera</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risen</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4\n",
       "        475  402  254  315  328\n",
       "used      0    1    0    0    0\n",
       "camera    0    0    0    0    1\n",
       "say       0    0    0    0    1\n",
       "risen     1    0    0    0    0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_table = pd.DataFrame(columns=range(len(docs)), index=list(vocabs))\n",
    "for v in vocabs:\n",
    "    for di in range(len(docs)):\n",
    "        d = docs[di]\n",
    "        freq = len(re.findall(v, d, flags=re.IGNORECASE))\n",
    "        tf_table.loc[v, di] = freq\n",
    "tf_table.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate inverted indexed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDM = {}\n",
    "for v in vocabs:\n",
    "    wposi = {}\n",
    "    for di in range(len(docs)):\n",
    "        d = docs[di]\n",
    "        matches = re.finditer(v, d, flags=re.IGNORECASE)\n",
    "        posis = []\n",
    "        for match in matches:\n",
    "            pid = match.start()\n",
    "            posis.append(pid)\n",
    "        wposi[di] = sorted(posis)\n",
    "    wposi['freq'] = len(wposi)\n",
    "    IDM[v] = wposi\n",
    "\n",
    "print(IDM['is'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
