{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"Climate change is accelerating faster than previously thought, \\\n",
    "      according to a new report from the Intergovernmental Panel on \\\n",
    "      Climate Change (IPCC). The report highlights that global \\\n",
    "      temperatures have risen by an average of 1.1°C since the late \\\n",
    "      19th century, with most of this increase occurring in the last 40 years. Rising sea levels, more frequent extreme weather events, and loss of biodiversity are some of the key impacts outlined in the report.\"\n",
    "d2 = \"The human genome consists of approximately 3 billion base pairs of DNA, which are organized into 23 pairs of chromosomes. Each gene contains the instructions necessary to build proteins, which are the molecular machines that carry out many functions in the body. The process of gene expression involves transcription, where DNA is copied into RNA, and translation, where RNA is used to build proteins.\"\n",
    "d3 = \"Person A: Hey, did you finish the report for the marketing team? \\\n",
    "      Person B: Not yet, I’m almost done. I just need to add a few more details. \\\n",
    "      Person A: Alright, make sure it’s submitted by 5 PM. \\\n",
    "      Person B: No problem, I’ve got it covered.\"\n",
    "d4 = \"n the year 1066, the Battle of Hastings took place in England, marking a pivotal moment in the country’s history. William the Conqueror, the Duke of Normandy, defeated King Harold II, which led to the Norman conquest of England. This event significantly influenced the culture, language, and governance of England.\"\n",
    "d5 = \"I recently bought the new XYZ smartphone, and I have to say, I’m really impressed. The camera quality is excellent, especially in low light. The battery life easily lasts a full day with heavy use, and the screen resolution is sharp and vibrant. My only complaint is the fingerprint sensor, which can be a bit finicky at times.\"\n",
    "\n",
    "\n",
    "docs = [d1, d2, d3, d4, d5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docs loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def docs_loader(corpus_dir:str):\n",
    "    fpaths = glob.iglob(os.path.join(corpus_dir, '*.txt'))\n",
    "    docs = []\n",
    "    for fp in fpaths:\n",
    "        with open(fp, 'r', encoding= \"utf-8\") as file:\n",
    "            doc = file.read()\n",
    "            docs.append(doc)\n",
    "    return docs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'used', 'camera', 'say', 'risen', 'more', 'battery', 'bought', 'rising', 'william', 'no', 'by', 'governance', '3', 'organized', 'outlined', 'problem', 'where', 'intergovernmental', 'are', 'moment', 'you', 'conquest', 'rna', 'billion', 'i', 'make', 'instructions', 'with', 'events', 'climate', 'out', 'proteins', 'it', 'details', 'year', '(', 'late', 'vibrant', 'bit', 'a', 'covered', 'influenced', 'expression', 'n', 'country’s', 'my', 'few', 'only', 'from', 'took', 'culture', 'times', 'gene', 'translation', 'machines', 'just', 'really', 'yet', 'for', 'since', 'this', 'not', 'history', 'into', 'ipcc', 'involves', 'life', '40', 'and', 'harold', 'report', 'some', 'excellent', 'norman', 'duke', 'significantly', 'sensor', 'according', 'biodiversity', 'sure', '23', '1°c', '5', 'did', 'occurring', 'it’s', 'got', 'marking', 'low', 'of', 'base', 'defeated', 'full', 'loss', 'average', 'weather', 'i’m', 'battle', 'key', 'transcription', 'on', 'century', 'chromosomes', 'alright', 'approximately', 'dna', 'contains', 'highlights', 'marketing', 'language', 'heavy', 'than', 'can', 'increase', 'new', 'molecular', 'carry', 'b', 'thought', 'use', 'impacts', 'person', 'complaint', 'levels', 'need', 'king', 'easily', 'fingerprint', 'in', 'to', 'finish', 'temperatures', 'frequent', 'process', 'copied', 'team', '19th', 'pm', 'be', 'england', 'which', 'submitted', 'pivotal', 'previously', 'sharp', 'impressed', 'is', 'lasts', ',', 'day', 'change', 'hey', '?', 'at', 'normandy', '1', '1066', 'functions', '.', 'extreme', 'done', 'most', ').', 'have', 'accelerating', ':', 'i’ve', 'xyz', 'pairs', 'led', 'faster', 'sea', 'build', 'hastings', 'genome', 'ii', 'event', 'the', 'almost', 'quality', 'especially', 'last', 'consists', 'body', 'finicky', 'many', 'an', 'years', 'necessary', 'light', 'resolution', 'that', 'place', 'add', 'conqueror', 'global', 'recently', 'smartphone', 'human', 'screen', 'panel', 'each'}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(docs:list[str]):\n",
    "    sep = r'(?<=\\w)(?=[,.!?:;\"\\'\\[\\]\\(\\)])|(?<=[,.!?:;\"\\'\\[\\]\\(\\)])(?=\\w)|\\s+'\n",
    "    pattern = re.compile(sep)\n",
    "    for d in docs:\n",
    "        words = pattern.split(d.strip().lower())\n",
    "        for w in words:\n",
    "            yield w\n",
    "\n",
    "\n",
    "#seen = set(tokenize(docs))\n",
    "#print(seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE (byte-pair encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_pair_counts(tokenized_doc:list[str]):\n",
    "    pair_counts = Counter()\n",
    "    for word in tokenized_doc:\n",
    "        chars = word.split()\n",
    "        for i in range(len(chars)-1):\n",
    "            pair = (chars[i],chars[i+1])\n",
    "            pair_counts[pair] += 1\n",
    "    return pair_counts\n",
    "\n",
    "\n",
    "def BPE_train_english(docs:list[str], numMerges:int = 500):\n",
    "    vocabs = set()\n",
    "    for d in docs:\n",
    "        vocabs.update(re.split(r\"\", d.strip().lower()))\n",
    "    if '' in vocabs:\n",
    "        vocabs.remove('')\n",
    "\n",
    "    word_tokens = []\n",
    "    for d in docs:\n",
    "        words = d.strip().lower().split()\n",
    "        word_tokens.extend([' '.join(w) for w in words])\n",
    "\n",
    "    merged_rule = []\n",
    "    for i in range(numMerges):\n",
    "        pair_counts = get_pair_counts(word_tokens)\n",
    "        if not pair_counts:\n",
    "            break\n",
    "        pair = pair_counts.most_common(1)[0][0]\n",
    "        vocabs.add(''.join(pair))\n",
    "\n",
    "        new_word_tokens = []\n",
    "        for word in word_tokens:\n",
    "            pattern = re.escape(\" \".join(pair))\n",
    "            merged_word = re.sub(pattern, \"\".join(pair), word)\n",
    "            new_word_tokens.append(merged_word)\n",
    "        \n",
    "        word_tokens = new_word_tokens\n",
    "        merged_rule.append(pair)\n",
    "    \n",
    "    \n",
    "    for word in word_tokens:\n",
    "        vocabs.update(word.split())\n",
    "\n",
    "    return vocabs, merged_rule\n",
    "\n",
    "\n",
    "\n",
    "def BPE_infer_english(phrase:str, merged_rule:list[tuple]):\n",
    "    tokenized_doc = [' '.join(w) for w in phrase.split()]\n",
    "    for pair in merged_rule:\n",
    "        pattern = re.escape(\" \".join(pair))\n",
    "        new_tokenized_doc = []\n",
    "        for word in tokenized_doc:\n",
    "            merged_word = re.sub(pattern, \"\".join(pair),word)\n",
    "            new_tokenized_doc.append(merged_word)\n",
    "            tokenized_doc = new_tokenized_doc\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "def BPE_train_chinese(docs:list[str], numMerge:int=500):\n",
    "    # initialize vocabs\n",
    "    vocabs = set()\n",
    "    for d in docs:\n",
    "        chars = list(d.strip())\n",
    "        vocabs.update(chars)\n",
    "    \n",
    "\n",
    "    # initialze split\n",
    "    tokenized_doc = []\n",
    "    for d in docs:\n",
    "        tokenized_doc.append(' '.join(d.strip()))\n",
    "        #print(' '.join(d.strip()))\n",
    "    \n",
    "    # merge words\n",
    "    merged_rule = []\n",
    "    for i in range(numMerge):\n",
    "        pair_counts = get_pair_counts(tokenized_doc)\n",
    "        if not pair_counts:\n",
    "            break\n",
    "        pair = pair_counts.most_common(1)[0][0]\n",
    "        merged_rule.append(pair)\n",
    "\n",
    "        # substitude\n",
    "        new_tokenized_doc = []\n",
    "        for tokenized_str in tokenized_doc:\n",
    "            merged_word = re.sub(re.escape(\" \".join(pair)), ''.join(pair), tokenized_str)            \n",
    "            new_tokenized_doc.append(merged_word)\n",
    "        tokenized_doc = new_tokenized_doc\n",
    "    \n",
    "    #print(tokenized_doc)\n",
    "\n",
    "\n",
    "    # update vocabs\n",
    "    for phrase in tokenized_doc:\n",
    "        vocabs.update(phrase.split())\n",
    "\n",
    "    return vocabs, merged_rule\n",
    "\n",
    "\n",
    "#print(BPE_infer(\"Alright, make sure it’s submitted by\", merged_rule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_len_n_subwords(word:str, n:int):\n",
    "    subwords = []\n",
    "    for i in range(len(word)-n+1):\n",
    "        ppp = []\n",
    "        for k in range(n):\n",
    "            ppp.append(word[i+k])\n",
    "        subwords.append(tuple(ppp))\n",
    "    return subwords\n",
    "\n",
    "def segmente(vocabs, )\n",
    "        \n",
    "\n",
    "def get_loss(more_gram:Counter, docs:list[str],removal_word:str= ''):\n",
    "    doc_size = len(docs)\n",
    "    tot_freq = more_gram.total()\n",
    "\n",
    "    \n",
    "    for i in range(doc_size):\n",
    "        summation = []\n",
    "\n",
    "        offset = more_gram[removal_word] if removal_word else 0\n",
    "\n",
    "        for pair, count in more_gram.items():\n",
    "            n_gram = ' '.join(pair)\n",
    "            if removal_word == n_gram:\n",
    "                continue\n",
    "            px = count/(tot_freq-offset)\n",
    "            summation.append(px)\n",
    "\n",
    "        inner = np.log10(sum(summation))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unigram_algo_english(docs:list[str], n:int):\n",
    "    # populate vocabs (unigram and more_gram)\n",
    "    tokenized_doc = tokenize(docs)\n",
    "    vocabs = set('abcdefghijklmnopqrstuvwxyz')\n",
    "    \n",
    "    more_gram = Counter()\n",
    "    for i in range(n-1):\n",
    "        length = i+2\n",
    "        # determine what-gram we looking for, and iterate to get those grams\n",
    "        for word in tokenized_doc:\n",
    "            #pairs = get_pairs(word, length)\n",
    "            more_gram.update()\n",
    "    \n",
    "    # pruning\n",
    "    # get total loss\n",
    "\n",
    "\n",
    "    # update final vocab\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'p', 'p'), ('p', 'p', 'l'), ('p', 'l', 'e')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'客', '些', '修', '颁', '天癸', '界', '地', '惰', '无论', '潮', '趣', '步', '延', '诗', '现', '保养', '来', '将', '四', '生活', '的，以', '及', '俱', '时，任', '专', '腾', '万物的', '，牙齿', '很', '上', '刚', '压', '蓬', '精神', '养', '天', '睡', '域', '构', '不会', '随着', '干', '量', '区', '泻', '索', '即', '怒', '供', '围', '标', '暖', '七', '命', '炫', '阴阳', '岁，肾气', '葵', '岁', '惑', '蕴', '真气', '，形体', '润', '盛，人们', '世俗', '。其', '变化', '辰', '会', '厌', '答', '女子', '五脏', '受', '助', '般', '世', '传', '生命', '耗散', '的', '热', '可以', '和', '追', '耳', '歌', '云', '患', '场', '元', '操', '生育', '明', '厥', '色', '活', '炼', '喜', '仍', '月', '怨', '乱', '息', '止', '弱', '后', '同', '沉', '的时候', '晚', '。如果', '圣人', '良', '神', '散', '春', '合', '肃', '展', '有子', '秩', '眼', '，精神', '筋', '置', '。如', '歧伯', '者，以', '酒', '列', '度', '纪', '立', '茂', '呼', '久', '样', '排', '危', '生育子女', '，以致', '暴', '入', '治', '房', '伤', '脱', '顺从', '绪', '伐', '由', '俗', '成', '务', '男', '恨', '伏', '密', '衰', '肺', '的变化', '滋', '呢', '让', '东', '精神，使', '防', '验', '痿', '龟', '迅', '创', '肉', '前', '软', '四时', '望', '稳', '闭', '等', '子', '去', '掘', '未', '悠', '它', '形体', '定', '情', '衰老', '1', '降', '舒', '绘', '文化', '互', '是', '通', '、', '国', '来，头发', '心', '再', '基', '重要', '勃', '速', '着', '持', '影', '本', '满', '交', '上古', '喧', '适应', '称', '衰弱', '事', '内', '冬', '键', '而不', '左', '精神，而', '生机', '已经', '井', '之人', '盖', '境', '人', '嚣', '免', '献', '岁时，肾气', '白', '风', '提供', '事物', '陈', '发', '独', '违', '励', '代', '阳气', '疾病', '社', '淫', '貌', '达', '扰', '吃', '宣', '不能', '解', '于', '倦', '把', '制', '黄', '教', '胸', '提', '\\U0010fc04', '竭', '给', '小', '敦', '脏', '英', '堕', '必', '十四', '精气', '脉', '的人，', '御', '不', '耀', '章', '露', '知', '依', '\\U0010fc03', '欣', '戏', '胀', '其', '智', '由于', '，这是', '以', '二', '惧', '统', '肝', '淳', '领', '从', '新', '运', '生', '假', '说:', '阴阳，', ' ', '怀', '丈', '谈', '拥', '吸', '能够', '求', '破', '，不能', '推', '，故', '肾', '业', '，以', '件', '加', '泛', '任', '惔', '动作', '，都', '勉', '幼', '锻', '深', '温', '闻', '眠', '乳', '开', '慎', '数', '度百岁', '我', '浆', '纯', '的，都', '。此', '倾', '清', '飧', '下', '策', '当', '造', '有称为', '床', '播', '象', '握', '时代', '听', '奖', '材', '耗', '家', '素', '昔', '山', '们', '超', '保', '庭', '科', '举', '随', '敏', '损', '恶', '措', '能', '愉', '音', '断', '挑', '杀', '疲', '。违逆了', '退', '肾气', '吗', '虚', '永', '帝', '年', '严', '歧', '今', '食', '经', '暂', '观', '往', '的人们', '发生', '槁', '甘', '光', '轻', '规律', '正', '说', '拒', '得', '理', '肖', '。二', '饮', '筛', '己', '强', '信息', '，\\U0010fc02', '善', '术', '终', '劣', '健康', '就不能', '恣', '的条件不足，', '旅', '，人们', '亦', '量的', '守', '\\U0010fc00地', '西', '自', '获', '慕', '计', '岐', '旺', '百', ';', '征', '还是', '除', '取', '最', '失', '什', '年龄', '出', '字', '余', '贤', '何', '全', '惩', '照', '势', '泽', '夜', '程', '华', '皮', '皆', '减', '折', '测', '识', '罚', '析', '违逆了', '动', '规', '纹', '台', '富', '避', '谨', '球', '阳', '质', '查', '，肾气', '夏', '医', '逆', '更', '脏，使提供给', '懂', '曰', '常', '响', '信', '服', '杂', '致', '觉', '畅', '带', '徇', '无', '师', '采', '回', '系', '缘', '鬓', '自然', '愚', '也', '分', '律', '产', '欲', '可', '机', '达到', '黄帝', '贡', '。', '与', '问', '引', '，不', '披', '媒', '但', '门', '十', '裂', '冲', '已', '辨', '悴', '整', '激', '低', '溢', '战', '冥', '浓', '游', '牙', '熟', '冷', '绵', '天地', '使', '为', '精', '升', '装', '苗', '化', '器', '乖', '此', '适', '癸', '迁', '值', '平', '市', '远', '憔', '爱', '竭，', '千', '尽', '贼', '虽', '属', '盛，', '筋骨', '落', '护', '他', '一', '牋', '注', '敝', '私', '改', '醉', '近', '伯', '才', '若', '，就会', '八', '岁时，', '昏', '限', '万', '归', '六', '题', '容', '肌', '亡', '种', '潜', '\\U0010fc02', '焦', '荣', '言', '气', '环', '高', '疗', '，少', '切', '纵', '足', '。无论', '既', '蛰', '繁', '赋', '木', '念', '动作不', '秋', '嗜', '季的', '美', '大', '收', '，筋骨', '中', '的方法', '外，这是', '身', '工', '优', '选', '面', '者，', '两', '雨', '，发', '符', '妄', '没', '腑', '草', '行', '多', '。三', '方', '实', '女', '瑜', '背', '根', '只', '形', '许', '也。', '渐', '气血', '邪', '植', '净', '累', '民', '害', '点', '史', '身体', '节', '阴', '接', '隆', '萌', '肤', '保养生', '九', '季的时', '这', '关', '川', '像', '城', '枯', '道', '万物', '之气', '秘', '则', '技', '历', '坏', '周', '贮', '乐', '敛', '?', '所以', '位', '愿', '对', '毫', '静', '还', '右', '懈', '变', '放', '雾', '居', '也。此', '秀', '病', '寿', '，起居', '劳', '疾', '进', '霾', '劲', '反', '融', '因', '功', '早', '收敛', '者', '志', '各', '饰', '三个月', '品', '夺', '却', '坚', '嗔', '意', '塞', '用', '\\U0010fc00癸', '离', '的时', '极', '时', '目', '要', '始', '帮', '向', '电', '五', '暗', '体', '齐', '紧', '真牙生', '伽', '百岁', '缓', '越', '心，', '积', '乎', '神，', '，人', '恼', '岐伯', '够', '到', '渴', '古', '到了', '重', '树', '半', '滥', '虑', '，是', '论', '宽', '尝', '应', '半百', '谓之', '算', '曰:', '学', '穿', '日', '掌握', '郁', '况', '益', '人们', '肌肉', '头', '结', '导', '蓄', '再去', '蒙', '耶', '思', '序', '性', '真人', '惯', '相', '便', '康', '预', '而不会', '，就', '泄', '漫', '急', '齿', '集', '冲脉', '又', '的人', '发展', '力', '至', '绝', '育', '检', '协', '施', '禾', '待', '均', '血', '沟', '养生之道', '的，', '的方', '都', '了', '令', '\\U0010fc00', '负', '疟', '寿命', '作', '好', '广', '习', '格', '别', '确', '总', '星', '少', '被', '僻', '快', '保持', '安', '隐', '办', '如', '头发', '书', '夭', '显', '法', '灾', '部', '弥', '竭，面', '笨', '调', '长', '景', '松', '养生', '在', '个', '据', '易', '增', '恬', '藏', '闲', '丰', '而', '恚', '具', '挈', '词', '岁时，筋骨', '驭', '时，', '络', '想', '物', '存', '矣', '，面', '乃', '院', '该', '候', '充', '水', '感', '能力', '嗜欲', '异', '朴', '式', '非', '壮', '视', '处', '外', '焕', '换', '登', '来，', '那', '，这', '画', '故', '?歧伯', '\\n', '鸡', '普', '冰', '勤', '老', '死', '尚', '金', '三', '时，人们', '担', '健', '阶', '，使', '就', '阔', '承', '兴', '择', '准', '超过', '频', '真', '蕃', '所', '消', '然', ':', '掌', '侵', '，故能', '麽', '。四十', '顺', '次', '临', '。四', '期', '驰', '需', '圣', '仿', '厚', '逐', '果', '决', '似', '主', '价', '外，', '衣', '，在', '探', '条', '享', '间', '四时，', '联', '勿', '做', '有', '，', '灵', '起', '龄', '兵', '按', '网', '惫', '违逆', '宁', '之', '段', '讲', '备', '盛', '特', '过', '叶', '季', '文', '寒', '，而', '没有', '自己的', '谓', '醇', '聪', '，头发', '德', '花', '晦', '，任', '开始', '快，', '，所以', '之人，', '骨'}\n"
     ]
    }
   ],
   "source": [
    "docs = docs_loader(\"./data\")\n",
    "vocabs , merged_rule = BPE_train_chinese(docs, 200)\n",
    "print(vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>475</td>\n",
       "      <td>402</td>\n",
       "      <td>254</td>\n",
       "      <td>315</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>camera</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risen</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4\n",
       "        475  402  254  315  328\n",
       "used      0    1    0    0    0\n",
       "camera    0    0    0    0    1\n",
       "say       0    0    0    0    1\n",
       "risen     1    0    0    0    0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_table = pd.DataFrame(columns=range(len(docs)), index=list(vocabs))\n",
    "for v in vocabs:\n",
    "    for di in range(len(docs)):\n",
    "        d = docs[di]\n",
    "        freq = len(re.findall(v, d, flags=re.IGNORECASE))\n",
    "        tf_table.loc[v, di] = freq\n",
    "tf_table.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate inverted indexed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDM = {}\n",
    "for v in vocabs:\n",
    "    wposi = {}\n",
    "    for di in range(len(docs)):\n",
    "        d = docs[di]\n",
    "        matches = re.finditer(v, d, flags=re.IGNORECASE)\n",
    "        posis = []\n",
    "        for match in matches:\n",
    "            pid = match.start()\n",
    "            posis.append(pid)\n",
    "        wposi[di] = sorted(posis)\n",
    "    wposi['freq'] = len(wposi)\n",
    "    IDM[v] = wposi\n",
    "\n",
    "print(IDM['is'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
